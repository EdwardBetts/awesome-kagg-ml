{
 "metadata": {
  "name": "",
  "signature": "sha256:4ba30d2e4dc85f80514835f5b94bae86204e33d148a69f545a592c1b37e3c10b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile otto_Pikki.py\n",
      "\n",
      "#%%writefile otto_Pikki.py\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.utils import shuffle\n",
      "#from sklearn.ensemble import RandomForestClassifier ,GradientBoostingClassifier\n",
      "#from sklearn.feature_selection import SelectKBest\n",
      "#from sklearn.feature_selection import chi2, f_classif\n",
      "#from sklearn.tree import DecisionTreeClassifier\n",
      "#from sklearn.pipeline import Pipeline\n",
      "#from sklearn.lda import LDA\n",
      "#from sklearn.neural_network import BernoulliRBM\n",
      "#from sklearn.svm import LinearSVC\n",
      "from sklearn import cross_validation\n",
      "#from sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n",
      "                           #       LassoLarsCV)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "def sigmoid(x):\n",
      "    print('XXXX')\n",
      "    y = 1/(1+np.exp(x))\n",
      "    return y\n",
      "#%matplotlib inline\n",
      "import time\n",
      "\n",
      "def load_data(train=True):\n",
      "    \"\"\"loads Data.\n",
      "    train == True means training data which has y values\n",
      "    train == False means testing (=pleasePredict) data, which only has X values\n",
      "    \"\"\"\n",
      "#test set\n",
      "    start = time.clock()\n",
      "\n",
      "    if train==False:\n",
      "        dp = '/home/thomas/Desktop/OttoChallenge/test.csv'\n",
      "        df = pd.read_csv(dp)\n",
      "        X = df.values.astype(np.float32)[:,1:]\n",
      "        return X\n",
      "#train set\n",
      "    else:\n",
      "        dp = '/home/thomas/Desktop/OttoChallenge/train.csv'\n",
      "        df = pd.read_csv(dp)\n",
      "        X = df[df.columns[:-1]].values.astype(np.float32)[:,1:]\n",
      "        y = df.target\n",
      "        y =y.apply(lambda X: int(X[-1])).values\n",
      "        y = y.astype(np.int32)\n",
      "        X, y = shuffle(X, y)\n",
      "        #print(X.shape,y.shape)\n",
      "        end = time.clock()\n",
      "        print(end-start)\n",
      "\n",
      "        return X,y\n",
      "\n",
      "\n",
      "def generate_features(X, shuffle_data = False, which_slice = 0,how_many_slices = 1,dummies = False):\n",
      "    \"\"\"generates features from np.array X.\n",
      "    most of them aren't that great but give a little boost in performance. Last 93 features are interesting,\n",
      "    computed from a correlation matrix.\n",
      "    \n",
      "    shuffle Data shuffles the data,\n",
      "    how many slices specifies by what number the data should be divided\n",
      "    which slice then states which part of the data should be used (mind the 0)\n",
      "    dummes keyword doesn't work yet.\n",
      "    \"\"\"\n",
      "    start = time.clock()\n",
      " \n",
      "    \n",
      "    df_X = pd.DataFrame(X)\n",
      "    if dummies ==True:\n",
      "        print('dummies')\n",
      "        \n",
      "    df_tmp = df_X.cumsum(axis=1)\n",
      "    \n",
      "    df_tmp_2 = df_X.cumprod(axis=1)\n",
      "    df_tmp_5 = df_X.cummax(axis=1)\n",
      "\n",
      "    df_tmp_6 = df_X.cummin(axis=1)\n",
      "    df_tmp_3 = df_X**2\n",
      "    df_tmp_4 =     df_X.T.diff(2).T.dropna(axis=1,how='any')\n",
      "#df_X**3\n",
      "    df_tmp_7 = df_X.T.diff(1).T.dropna(axis=1,how='any')\n",
      "    df_tmp_8 = df_X**3\n",
      "    df_tmp_9 = df_X**5\n",
      "    df_tmp_10 = df_X - df_X.apply(np.mean,axis=0)#will this work?\n",
      "    df_test = pd.stats.moments.rolling_mean(df_X.T,window = 19,axis=1)\n",
      "    df_test = df_test.T.dropna(axis=1,how='any')\n",
      "\n",
      "    df_X['nonzero_sum'] = df_X.apply(np.count_nonzero,axis=1)\n",
      "   # df_X['binc'] = df_X.apply(np.unique,axis=1)\n",
      "    corr = df_X.corr()\n",
      "    df_tmp_11 = df_X.dot(corr)\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    df_X['mean'] = df_X.apply(np.mean,axis=1,raw=True)\n",
      "    df_X['std'] = df_X.apply(np.std,axis=1,raw=True)\n",
      "    df_X['median'] = df_X.apply(np.median,axis=1,raw=True)\n",
      "    df_X['amax'] = df_X.apply(np.amax,axis=1,raw=True)\n",
      "    df_X['amin'] = df_X.apply(np.amin,axis=1,raw=True)\n",
      "\n",
      "\n",
      "    df_X['ptp'] = df_X.apply(np.ptp,axis=1)\n",
      "    \n",
      "    \n",
      "    df_X = pd.concat([df_X,\n",
      "                      df_tmp,\n",
      "                      df_tmp_2,\n",
      "                      #df_tmp_3,\n",
      "                      df_tmp_4,\n",
      "                      df_tmp_5,\n",
      "                      df_tmp_6,\n",
      "                      #df_tmp_7,\n",
      "                      #df_tmp_8,\n",
      "                      #df_tmp_9,\n",
      "              df_tmp_11,\n",
      "                      df_test\n",
      "                      ],axis=1)\n",
      "        \n",
      "    X = df_X.values.astype(np.float32)\n",
      "\n",
      "    if shuffle_data == True:\n",
      "        z = shuffle(X.T)\n",
      "        X = z.T\n",
      "    print('shape')\n",
      "    print(X.shape)\n",
      "    \n",
      "    slice_length = round(X.shape[1]/how_many_slices)\n",
      "    print(slice_length, how_many_slices)\n",
      "    X = X[:,which_slice*slice_length:(which_slice+1)*slice_length]\n",
      "    \n",
      "    print('shape new')\n",
      "    print(X.shape)\n",
      "    end = time.clock()\n",
      "\n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def get_score(clf, features = True,which_slice = 0,how_many_slices = 1,shuffle_data = False):\n",
      "    \"\"\"computes a cross validation score\n",
      "    \n",
      "    takes all keywords from generate_features\"\"\"\n",
      "    \n",
      "    start = time.clock()\n",
      "    X,y = load_data()\n",
      "    if features == True:\n",
      "        X = generate_features(X,which_slice=which_slice,how_many_slices=how_many_slices ,shuffle_data=shuffle_data)\n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size=0.2)\n",
      "    #print(y_train)\n",
      "    clf.fit(X_train,y_train)\n",
      "    outcome = clf.predict_proba(X_test)\n",
      "    end = time.clock()\n",
      "    print('SCORE: ' , log_loss(y_test,outcome), ' time: ',end-start)\n",
      "    \n",
      "def get_score2(clf,features = True,which_slice = 1,how_many_slices = 5,shuffle_data = False):\n",
      "    X,y = load_data()\n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size=0.2)#, random_state=0)\n",
      "    clf.fit(X_train,y_train)\n",
      "    outcome = clf.predict_proba(X_test)\n",
      "    print(log_loss(y_test,outcome))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def make_guess_csv(clf,features =True,name = 'test',which_slice = 0,how_many_slices = 1,shuffle_data = False):\n",
      "    \"\"\"generates a csv file containing the output from one classifier.\n",
      "    index starts at 0, so no submittable file is created (was necessary in an older version of combine_results, \n",
      "    not sure if it still is)\n",
      "    \"\"\"\n",
      "    start = time.clock()\n",
      "    X_test,y_test = load_data()\n",
      "    X = load_data(train=False)\n",
      "    if features == True:\n",
      "        X = generate_features(X,which_slice=which_slice ,how_many_slices=how_many_slices ,shuffle_data=shuffle_data)\n",
      "        X_test = generate_features(X_test,which_slice=which_slice,how_many_slices=how_many_slices ,shuffle_data=shuffle_data )\n",
      "\n",
      "    \n",
      "    clf.fit(X_test,y_test)\n",
      "    output = clf.predict_proba(X)\n",
      "    \n",
      "    \n",
      "    cols = ['Class_1',\n",
      "      'Class_2',\n",
      "      'Class_3',\n",
      "      'Class_4',\n",
      "      'Class_5',\n",
      "      'Class_6',\n",
      "      'Class_7',\n",
      "      'Class_8',\n",
      "      'Class_9']\n",
      "    df_write = pd.DataFrame(output)\n",
      "    shuf = '_noshuf_'\n",
      "    if shuffle_data==True:\n",
      "        shuf = '_shufled_'\n",
      "    slices = 'num_slice_' + str(how_many_slices)\n",
      "    which_clf = clf\n",
      "    num_slice = 'slice_num_' + str(which_slice)\n",
      "    clf = str(clf)\n",
      "    \n",
      "    file_name = 'Pikki' + name  +num_slice + slices + shuf + '.csv'\n",
      "    print('written to ' + file_name)\n",
      "    df_write.to_csv(file_name,header = cols,index_label = ['id'])\n",
      "    end = time.clock()\n",
      "    print(end-start)\n",
      "    identifier = name  +num_slice + slices + shuf\n",
      "    return identifier\n",
      "    \n",
      "\n",
      "\n",
      "def load_all_dfs(clf_list = ['test_small','rt_small','test2_small']):\n",
      "    \"\"\"loads all the csv files that end with specification in clf_list\n",
      "    in multiindex DataFrame. first row is specification, second is index\n",
      "    \n",
      "    helper for combine_results and plot_bias\n",
      "    \"\"\"\n",
      "    \n",
      "    start = time.clock()\n",
      "    print('loading data')\n",
      "    first_clf = clf_list[0]\n",
      "    df = pd.read_csv('Pikki'+first_clf+'.csv')\n",
      "    df['df'] = first_clf\n",
      "\n",
      "    df = df.set_index(['id','df'])\n",
      "\n",
      "    for clf in clf_list[1:]:\n",
      "        file_name = 'Pikki' + clf + '.csv'\n",
      "        df_tmp = pd.read_csv(file_name)\n",
      "        df_tmp['df'] = clf\n",
      "\n",
      "        df_tmp = df_tmp.set_index(['id','df'])\n",
      "\n",
      "        df = pd.concat([df,df_tmp])\n",
      "\n",
      "        \n",
      "    df['std'] = df.apply(np.std,axis=1,raw = True)\n",
      "    end = time.clock()\n",
      "    print(end-start)\n",
      "    return df#.swaplevel(0,1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def combine_results(voting = 'hard',clf_list = ['test_small','rt_small','test2_small']):\n",
      "    \"\"\"combines two or more classifier outputs. \n",
      "    by now only uses hard voting by standard deviation \n",
      "    (more std means higher values which ~ means higher confidence)\n",
      "    other measures welcome\n",
      "    \n",
      "    creates both a submittable file (\"combined ...\")\n",
      "    and a file that can be used for combination with other classifiers\n",
      "    \"\"\"\n",
      "    \n",
      "    start = time.clock()\n",
      "    df = load_all_dfs(clf_list)\n",
      "\n",
      "    print('combining the data and voting ', voting)\n",
      "\n",
      "    if voting == 'hard':\n",
      "        print('voting')\n",
      "\n",
      "        label_tupel_list = list(df.groupby(level=['id'])['std'].idxmax())#idmax \n",
      "        num_samples = len(label_tupel_list)\n",
      "        index = [label_tupel_list[i][0] for i in range(num_samples)]\n",
      "        df.index\n",
      "        time_need = []\n",
      "        t2 = 0\n",
      "\n",
      "        print(\"doing god's work\")\n",
      "        df_new = df.ix[index]\n",
      "        df_new = df.ix[label_tupel_list]\n",
      "    end = time.clock()\n",
      "    print('done', end-start)\n",
      "    #return df_new\n",
      "   \n",
      "    \n",
      "    cols = ['Class_1',\n",
      "      'Class_2',\n",
      "      'Class_3',\n",
      "      'Class_4',\n",
      "      'Class_5',\n",
      "      'Class_6',\n",
      "      'Class_7',\n",
      "      'Class_8',\n",
      "      'Class_9']\n",
      "    df_new2 = df_new.reset_index()\n",
      "    del df_new2['std']\n",
      "    del df_new2['id']\n",
      "    del df_new2['df']\n",
      "\n",
      "    print('zero')\n",
      "    try:\n",
      "     print('first')\n",
      "     clf_names = 'with_'\n",
      "     print('second')\n",
      "     for i in range(len(clf_list)):\n",
      "         print(clf_list[i])\n",
      "         clf_names = clf_names + '_' +  clf_list[i]\n",
      "            \n",
      "     df_new2.to_csv('Pikki'+clf_names+ '.csv',header = cols,index_label = ['id'])\n",
      "       \n",
      "     df_new2.index +=1\n",
      "\n",
      "     print('written to')\n",
      "     print('Pikki'+clf_names+ '.csv')\n",
      "     \n",
      "     df_new2.to_csv('combined_Pikki'+clf_names+ '.csv',header = cols,index_label = ['id'])\n",
      "    except:\n",
      "        df_new2.to_csv('combined_Pikki.csv',header = cols,index_label = ['id'])\n",
      "    return df_new    \n",
      "\n",
      "\n",
      "\n",
      "def plot_bias(clf_list = ['test_small','rt_small','test2_small'],return_df = False,XKCD = False):\n",
      "    \"\"\"plots some differences between two output files.\n",
      "    right now plots for every class and every classifier\n",
      "    \n",
      "    mean\n",
      "    max\n",
      "    std\n",
      "    diff(clfx-clfy)\n",
      "    \n",
      "    plt.xkcd() is optional.\n",
      "    \"\"\"\n",
      "    if XKCD == True:\n",
      "        plt.xkcd()\n",
      "    print('damn')\n",
      "    df = load_all_dfs(clf_list)\n",
      "    df = df.swaplevel(0,1)\n",
      "    del df['std']\n",
      "    df.hist()\n",
      "    plt.figure()\n",
      "\n",
      "    for clf in clf_list:\n",
      "        df.ix[clf].mean().plot(label = clf,figsize=(16, 4))\n",
      "    plt.legend(loc='upper right')\n",
      "    plt.title('mean')\n",
      "    plt.figure()\n",
      "    \n",
      "   # c = df.columns\n",
      "    for clf in clf_list:\n",
      "        #df[c[1:]].ix[clf].max().plot(label = clf,figsize=(16, 4))\n",
      "        df.ix[clf].max().plot(label = clf,figsize=(16, 4))\n",
      "    plt.legend(loc='upper right')\n",
      "    plt.title('max')\n",
      "    \n",
      "    plt.figure()\n",
      "    for clf in clf_list:\n",
      "        df.ix[clf].std().plot(label = clf,figsize=(16, 4))\n",
      "\n",
      "        \n",
      "    plt.legend(loc='upper right')\n",
      "    plt.title('std')\n",
      "    plt.figure()\n",
      "    used_list = []\n",
      "    for clf in clf_list:\n",
      "        for clf2 in clf_list:\n",
      "            if (clf != clf2) and ({clf,clf2} not in used_list):\n",
      "                diff = ((df.ix[clf] - df.ix[clf2])**2)**(1/2)\n",
      "                diff.mean().plot(label = clf+' - ' +clf2,figsize=(16, 4))\n",
      "                used_list.append({clf,clf2})\n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "    plt.legend(loc='upper right')\n",
      "    plt.title('difference')\n",
      "    print('damnover')\n",
      "    if return_df == True:\n",
      "     return df\n",
      "    \n",
      "def example():\n",
      "    \"\"\"example on how to run this\"\"\"\n",
      "    #generate a classifier\n",
      "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "    clf = RandomForestClassifier(n_estimators=4,n_jobs=-1)\n",
      "    clf2 = GradientBoostingClassifier(n_estimators = 1)\n",
      "    #make guess csv\n",
      "    n1 = make_guess_csv(clf,name='example')\n",
      "    n2 = make_guess_csv(clf2,name='example_2')\n",
      "    #combine results\n",
      "    print(n1,n2)\n",
      "    combine_results(clf_list=[n1,n2])\n",
      "    #plot the stuff\n",
      "    plot_bias([n1,n2],XKCD = True)\n",
      "    \n",
      "#noclue if this is necessary. If somebody has better information, please let me know\n",
      "def main():\n",
      "    #example()\n",
      "    print('something')\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-2-4e2dff8fa4e8>, line 403)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-4e2dff8fa4e8>\"\u001b[1;36m, line \u001b[1;32m403\u001b[0m\n\u001b[1;33m    if __name__ == \"__main__\":\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}